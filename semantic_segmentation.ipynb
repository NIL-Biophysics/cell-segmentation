{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lQVlL0_wqKw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "    DEVICE = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "images = []\n",
        "lesions = []\n",
        "import os\n",
        "from skimage.io import imread\n",
        "root = 'dataset_cut_6'\n",
        "\n",
        "for root, dirs, files in os.walk(os.path.join(root)):\n",
        "    if root.endswith('image'):\n",
        "        images.append(imread(os.path.join(root, files[0])))\n",
        "    if root.endswith('lesion'):\n",
        "        lesions.append(imread(os.path.join(root, files[0])))"
      ],
      "metadata": {
        "id": "w02wfaAtUJwQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "from skimage.transform import resize\n",
        "size = (256, 256)\n",
        "X = [resize(x, size, mode='constant', anti_aliasing=True,) for x in images]\n",
        "Y = [resize(y, size, mode='constant', anti_aliasing=False) for y in lesions]"
      ],
      "metadata": {
        "id": "CuvscZsyUJwQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X = np.array(X, np.float32)\n",
        "Y = np.array(Y, np.float32)\n",
        "print(f'Loaded {len(X)} images')\n",
        "print(f'Loaded {len(Y)} images')"
      ],
      "metadata": {
        "id": "fWo5ZjA6UJwR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iS3oK0hTbpX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "for i in range(6):\n",
        "    plt.subplot(2, 6, i+1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(X[i])\n",
        "\n",
        "    plt.subplot(2, 6, i+7)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(Y[i])\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "imqjeizbTbpe"
      },
      "outputs": [],
      "source": [
        "ix = np.random.choice(len(X), len(X), False)\n",
        "tr, val, ts = np.split(ix, [348, 522])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-uxCdAKTbpi"
      },
      "outputs": [],
      "source": [
        "print(len(tr), len(val), len(ts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EHYLt5eqTbpp"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 32\n",
        "data_tr = DataLoader(list(zip(np.rollaxis(X[tr], 3, 1), Y[tr, np.newaxis])),\n",
        "                     batch_size=batch_size, shuffle=True)\n",
        "data_val = DataLoader(list(zip(np.rollaxis(X[val], 3, 1), Y[val, np.newaxis])),\n",
        "                      batch_size=batch_size, shuffle=True)\n",
        "data_ts = DataLoader(list(zip(np.rollaxis(X[ts], 3, 1), Y[ts, np.newaxis])),\n",
        "                     batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "from time import time\n",
        "\n",
        "from matplotlib import rcParams\n",
        "rcParams['figure.figsize'] = (15,4)"
      ],
      "metadata": {
        "id": "q-jjxWMQUJwR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pkr8g4ghFxox"
      },
      "outputs": [],
      "source": [
        "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
        "    # You can comment out this line if you are passing tensors of equal shape\n",
        "    # But if you are passing output from UNet or something it will most probably\n",
        "    # be with the BATCH x 1 x H x W shape\n",
        "    outputs = outputs.squeeze(1).byte()  # BATCH x 1 x H x W => BATCH x H x W\n",
        "    labels  = labels.squeeze(1).byte()   # Forming outputs to the correct form\n",
        "    SMOOTH = 1e-8\n",
        "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
        "    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n",
        "\n",
        "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
        "\n",
        "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
        "\n",
        "    return thresholded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HgF8S8IZTbqE"
      },
      "outputs": [],
      "source": [
        "def bce_loss(y_real, y_pred):\n",
        "    loss = y_pred - y_real*y_pred + (1+ torch.exp(-1*y_pred)).log()\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "source": [
        "def train(model, optimizer, scheduler, loss_fn, score_fn, epochs, data_tr, data_vl, device):\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    losses_train = []\n",
        "    losses_val = []\n",
        "    scores_train = []\n",
        "    scores_val = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        tic = time()\n",
        "        print('* Epoch %d/%d' % (epoch+1, epochs))\n",
        "\n",
        "        avg_loss = 0\n",
        "        model.train()  # train mode\n",
        "        for X_batch, Y_batch in data_tr:\n",
        "            # data to device\n",
        "            X_batch = X_batch.to(device)\n",
        "            Y_batch = Y_batch.to(device)\n",
        "\n",
        "            # set parameter gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            Y_pred = model(X_batch)\n",
        "            #print(\"Y_pred len: \", Y_pred.size())\n",
        "            #print(\"Y_batch len: \", Y_batch.size())\n",
        "            loss = loss_fn(Y_pred, Y_batch) # forward-pass\n",
        "\n",
        "            loss.backward()  # backward-pass\n",
        "            optimizer.step()  # update weights\n",
        "\n",
        "            # calculate loss to show the user\n",
        "            avg_loss += loss / len(data_tr)\n",
        "\n",
        "        toc = time()\n",
        "        print('train_loss: %f' % avg_loss)\n",
        "        losses_train.append(avg_loss)\n",
        "\n",
        "        # train score\n",
        "        avg_score_train = score_fn(model, iou_pytorch, data_tr)\n",
        "        scores_train.append(avg_score_train)\n",
        "\n",
        "        # val loss\n",
        "        avg_loss_val = 0\n",
        "        model.eval()  # testing mode\n",
        "        for X_val, Y_val in data_vl:\n",
        "            with torch.no_grad():\n",
        "                Y_hat = model(X_val.to(device)).detach().cpu()# detach and put into cpu\n",
        "\n",
        "                loss = loss_fn(Y_hat, Y_val) # forward-pass\n",
        "                avg_loss_val += loss / len(data_vl)\n",
        "\n",
        "        toc = time()\n",
        "        print('val_loss: %f' % avg_loss_val)\n",
        "        losses_val.append(avg_loss_val)\n",
        "\n",
        "        # val score\n",
        "        avg_score_val = score_fn(model, iou_pytorch, data_vl)\n",
        "        scores_val.append(avg_score_val)\n",
        "\n",
        "        if scheduler:\n",
        "            #scheduler.step(avg_score_val)\n",
        "            scheduler.step()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return (losses_train, losses_val, scores_train, scores_val)"
      ],
      "metadata": {
        "id": "Vqv1NkTHUJwS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ArU3i1i8Tbqm"
      },
      "outputs": [],
      "source": [
        "def predict(model, data):\n",
        "    model.eval()  # testing mode\n",
        "    Y_pred = [ X_batch.to(DEVICE) for X_batch, _ in data]\n",
        "    return np.array(Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "H_L7JNgnJfGA"
      },
      "outputs": [],
      "source": [
        "def score_model(model, metric, data):\n",
        "    model.eval()  # testing mode\n",
        "    scores = 0\n",
        "    for X_batch, Y_label in data:\n",
        "        with torch.no_grad():\n",
        "            X_batch = X_batch.to(DEVICE)\n",
        "            Y_batch = Y_label.to(DEVICE)\n",
        "            Y_pred = model(X_batch)\n",
        "\n",
        "            #We need to make outputs in the range from 0 to 1, as masks are\n",
        "            #If output is bigger than treshhold level => it is 1, else - zero\n",
        "            #Treshold is 0.1\n",
        "\n",
        "            # 'torch.ones_like' returns the tensor with the size like the input matrix size\n",
        "            Y_pred = torch.ones_like(Y_pred) * (Y_pred > 0.1)\n",
        "            scores += metric(Y_pred, Y_label.to(DEVICE)).mean().item()\n",
        "\n",
        "    return scores/len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bzcnyua4UJwT"
      },
      "outputs": [],
      "source": [
        "# Function to build training plots\n",
        "def plot_results(rslts, ttl=''):\n",
        "    # This part is needed to proceed the error with torch.no_grad\n",
        "    with torch.no_grad():\n",
        "        plt.figure(figsize=(15, 9))\n",
        "        plt.plot(rslts[0], label=\"train_loss\")\n",
        "        plt.plot(rslts[1], label=\"val_loss\")\n",
        "        plt.plot(rslts[2], label=\"train_score\")\n",
        "        plt.plot(rslts[3], label=\"val_score\")\n",
        "\n",
        "        plt.legend(loc='best')\n",
        "        plt.xlabel(\"epochs\")\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.title(ttl)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dpCi38-QTbrz"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_real, y_pred):\n",
        "\n",
        "    smooth = 1e-8\n",
        "    outputs = y_pred.sigmoid().squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
        "    labels = y_real.squeeze(1)\n",
        "\n",
        "    num = (outputs * labels).sum()\n",
        "    den = (outputs + labels).sum()\n",
        "    res = 1 - ((2. * num + smooth) / (den + smooth))#/(256*256)\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PLogsetCTbsK"
      },
      "outputs": [],
      "source": [
        "def focal_loss(y_real, y_pred, eps = 1e-8, gamma = 2):\n",
        "    y = y_pred.sigmoid()+eps\n",
        "    loss = -((1-y)**gamma*y_real*y.log()+(1-y_real)*(1-y).log())\n",
        "    return loss.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLKGrI4YTbs9"
      },
      "outputs": [],
      "source": [
        "# Вспомогательный класс. Реализация синего слоя с картинки выше\n",
        "\n",
        "class conv2DBatchNormRelu(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters, k_size, stride, padding):\n",
        "        super(conv2DBatchNormRelu, self).__init__()\n",
        "\n",
        "        self.unit = nn.Sequential(\n",
        "            nn.Conv2d(int(in_channels), int(n_filters), kernel_size=k_size, padding=padding, stride=stride),\n",
        "            nn.BatchNorm2d(int(n_filters)),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.unit(inputs)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # encoder (downsampling)\n",
        "        self.enc_conv0 = nn.Sequential(\n",
        "            conv2DBatchNormRelu(3, 64, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(64, 64, 3, 1, 1)\n",
        "        )\n",
        "        self.pool0 = nn.MaxPool2d(2, 2, return_indices=True)  # 256 -> 128\n",
        "\n",
        "        self.enc_conv1 = nn.Sequential(\n",
        "            conv2DBatchNormRelu(64, 128, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(128, 128, 3, 1, 1),\n",
        "        )\n",
        "        self.pool1 = nn.MaxPool2d(2, 2, return_indices=True) # 128 -> 64\n",
        "\n",
        "        self.enc_conv2 = nn.Sequential(\n",
        "            conv2DBatchNormRelu(128, 256, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(256, 256, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(256, 256, 3, 1, 1)\n",
        "        )\n",
        "        self.pool2 = nn.MaxPool2d(2, 2, return_indices=True) # 64 -> 32\n",
        "\n",
        "        self.enc_conv3 = nn.Sequential(\n",
        "            conv2DBatchNormRelu(256, 512, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(512, 512, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(512, 512, 3, 1, 1)\n",
        "        )\n",
        "        self.pool3 = nn.MaxPool2d(2, 2, return_indices=True) # 32 -> 16\n",
        "\n",
        "        self.bottle_neck = nn.Sequential(\n",
        "            conv2DBatchNormRelu(512, 1024, 1, 1, 0),\n",
        "            conv2DBatchNormRelu(1024, 512, 1, 1, 0)\n",
        "        )\n",
        "\n",
        "        self.upsample3 = nn.MaxUnpool2d(2, 2) # 16 -> 32\n",
        "        self.dec_conv3 = nn.Sequential(\n",
        "            conv2DBatchNormRelu(512*2, 256, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(256, 256, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(256, 256, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "        self.upsample2 = nn.MaxUnpool2d(2, 2) # 32 -> 64\n",
        "        self.dec_conv2 = nn.Sequential(\n",
        "            conv2DBatchNormRelu(256*2, 128, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(128, 128, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(128, 128, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "        self.upsample1 = nn.MaxUnpool2d(2, 2) # 64 -> 128\n",
        "        self.dec_conv1 = nn.Sequential(\n",
        "            conv2DBatchNormRelu(128*2, 64, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(64, 64, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "        self.upsample0 = nn.MaxUnpool2d(2, 2) # 128 -> 256\n",
        "        self.dec_conv0 = nn.Sequential(\n",
        "            conv2DBatchNormRelu(64*2, 1, 3, 1, 1),\n",
        "            conv2DBatchNormRelu(1, 1, 3, 1, 1),\n",
        "\n",
        "            nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(1),\n",
        "            # nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder\n",
        "        pre_e0 = self.enc_conv0(x)\n",
        "        e0, ind0 = self.pool0(pre_e0)\n",
        "        pre_e1 = self.enc_conv1(e0)\n",
        "        e1, ind1 = self.pool1(pre_e1)\n",
        "        pre_e2 = self.enc_conv2(e1)\n",
        "        e2, ind2 = self.pool2(pre_e2)\n",
        "        pre_e3 = self.enc_conv3(e2)\n",
        "        e3, ind3 = self.pool3(pre_e3)\n",
        "\n",
        "        # bottleneck\n",
        "        bottle_neck = self.bottle_neck(e3)\n",
        "\n",
        "        # decoder\n",
        "        d3 = self.dec_conv3(torch.cat([self.upsample3(bottle_neck, ind3), pre_e3], 1))\n",
        "        d2 = self.dec_conv2(torch.cat([self.upsample2(d3, ind2), pre_e2], 1))\n",
        "        d1 = self.dec_conv1(torch.cat([self.upsample1(d2, ind1), pre_e1], 1))\n",
        "        d0 = self.dec_conv0(torch.cat([self.upsample0(d1, ind0), pre_e0], 1))\n",
        "\n",
        "        # no activation\n",
        "        return d0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15GA_43BTbtI"
      },
      "outputs": [],
      "source": [
        "unet_model = UNet().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5s-jQY4TbtZ"
      },
      "outputs": [],
      "source": [
        "max_epochs = 60\n",
        "optimizer = torch.optim.AdamW(unet_model.parameters(), lr=0.00100, weight_decay=0.05)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20, 40], gamma=0.7)\n",
        "results_unet = train(unet_model, optimizer, scheduler, focal_loss, score_model, max_epochs, data_tr, data_val, DEVICE)\n",
        "results_unet = ([t.cpu() for t in results_unet[0]],\n",
        "                        [t.cpu() for t in results_unet[1]],\n",
        "                        [t for t in results_unet[2]],\n",
        "                        [t for t in results_unet[3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErfzGXZdNcWQ"
      },
      "outputs": [],
      "source": [
        "score_model(unet_model, bce_loss, data_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDlieAm0UJwU"
      },
      "outputs": [],
      "source": [
        "plot_results(results_unet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JVAtD4RvUJwU"
      },
      "outputs": [],
      "source": [
        "def plot_items(model, loader, cnt):\n",
        "\n",
        "    model.eval()\n",
        "    X, Y = next(iter(loader))\n",
        "    X = X.to(DEVICE)\n",
        "    Y = Y.to(DEVICE)\n",
        "    Y_pred = model(X)\n",
        "\n",
        "    p = Y_pred.detach().cpu()\n",
        "    p_post = torch.ones_like(p) * (p > 0.1)\n",
        "    y = Y.detach().cpu()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(cnt):\n",
        "        plt.subplot(3, cnt, i+1+cnt*0)\n",
        "        plt.imshow(np.rollaxis(p[i,0].numpy(), 0), cmap='gray')\n",
        "        plt.title('Output')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(3, cnt, i+1+cnt*1)\n",
        "        plt.imshow(np.rollaxis(p_post[i,0].numpy(), 0), cmap='gray')\n",
        "        plt.title('Post-processing')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(3, cnt, i+1+cnt*2)\n",
        "        plt.imshow(np.rollaxis(y[i,0].numpy(), 0), cmap='gray')\n",
        "        plt.title('Real')\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdvQVc34UJwU"
      },
      "outputs": [],
      "source": [
        "plot_items(unet_model, data_ts, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7FCgUrRNTbth"
      },
      "outputs": [],
      "source": [
        "class UNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # encoder (downsampling)\n",
        "        self.enc_conv0 = nn.Sequential(\n",
        "             nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(64),\n",
        "             nn.ReLU(),\n",
        "\n",
        "             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(64),\n",
        "             nn.ReLU(),\n",
        "\n",
        "             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(64),\n",
        "             nn.ReLU()\n",
        "        )\n",
        "        self.pool0 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.enc_conv1 = nn.Sequential(\n",
        "             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(128),\n",
        "             nn.ReLU(),\n",
        "\n",
        "             nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(128),\n",
        "             nn.ReLU(),\n",
        "\n",
        "             nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(128),\n",
        "             nn.ReLU()\n",
        "        )\n",
        "        self.pool1 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1) # 128 -> 64\n",
        "\n",
        "        self.enc_conv2 = nn.Sequential(\n",
        "             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(256),\n",
        "             nn.ReLU(),\n",
        "\n",
        "             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(256),\n",
        "             nn.ReLU(),\n",
        "\n",
        "             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(256),\n",
        "             nn.ReLU()\n",
        "        )\n",
        "        self.pool2 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1) # 64 -> 32\n",
        "\n",
        "        self.enc_conv3 = nn.Sequential(\n",
        "             nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(512),\n",
        "             nn.ReLU(),\n",
        "\n",
        "             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(512),\n",
        "             nn.ReLU(),\n",
        "\n",
        "             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(512),\n",
        "             nn.ReLU()\n",
        "        )\n",
        "        self.pool3 = nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1) # 32 -> 16\n",
        "\n",
        "        self.bottle_neck = nn.Sequential(\n",
        "             nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(1024),\n",
        "             nn.ReLU(),\n",
        "\n",
        "             nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, padding=1, stride=1),\n",
        "             nn.BatchNorm2d(512),\n",
        "             nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.upsample3 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1) # 16 -> 32\n",
        "        self.dec_conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=512*2, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.upsample2 = nn.ConvTranspose2d(256, 256, kernel_size=3, stride=2, padding=1) # 32 -> 64\n",
        "        self.dec_conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=256*2, out_channels=128, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.upsample1 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1) # 64 -> 128\n",
        "        self.dec_conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128*2, out_channels=64, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.upsample0 = nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1) # 128 -> 256\n",
        "        self.dec_conv0 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64*2, out_channels=32, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, stride=1),\n",
        "            nn.BatchNorm2d(1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder\n",
        "        pre_e0 = self.enc_conv0(x)\n",
        "        e0 = self.pool0(pre_e0)\n",
        "        pre_e1 = self.enc_conv1(e0)\n",
        "        e1 = self.pool1(pre_e1)\n",
        "        pre_e2 = self.enc_conv2(e1)\n",
        "        e2 = self.pool2(pre_e2)\n",
        "        pre_e3 = self.enc_conv3(e2)\n",
        "        e3 = self.pool3(pre_e3)\n",
        "\n",
        "        # bottleneck\n",
        "        bottle_neck = self.bottle_neck(e3)\n",
        "\n",
        "        # decoder\n",
        "        d3 = self.dec_conv3(torch.cat([self.upsample3(bottle_neck, output_size=pre_e3.size()), pre_e3], 1))\n",
        "        d2 = self.dec_conv2(torch.cat([self.upsample2(d3, output_size=pre_e2.size()), pre_e2], 1))\n",
        "        d1 = self.dec_conv1(torch.cat([self.upsample1(d2, output_size=pre_e1.size()), pre_e1], 1))\n",
        "        d0 = self.dec_conv0(torch.cat([self.upsample0(d1, output_size=pre_e0.size()), pre_e0], 1))\n",
        "        # no activation\n",
        "        return d0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GGjYsmOqTbtm"
      },
      "outputs": [],
      "source": [
        "unet2_model = UNet2().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMSZlETGTbts"
      },
      "outputs": [],
      "source": [
        "max_epochs = 60\n",
        "optimizer = torch.optim.AdamW(unet2_model.parameters(), lr=0.00100, weight_decay=0.05)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20,30,40], gamma=0.7)\n",
        "results_unet = train(unet2_model, optimizer, scheduler, bce_loss, score_model, max_epochs, data_tr, data_val, DEVICE)\n",
        "results_unet = ([t.cpu() for t in results_unet[0]],\n",
        "                        [t.cpu() for t in results_unet[1]],\n",
        "                        [t for t in results_unet[2]],\n",
        "                        [t for t in results_unet[3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xcoZIt4UJwV"
      },
      "outputs": [],
      "source": [
        "score_model(unet2_model, bce_loss, data_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dIq1Q0tUJwY"
      },
      "outputs": [],
      "source": [
        "plot_items(unet2_model, data_ts, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PUyZxt4UJwY"
      },
      "outputs": [],
      "source": [
        "plot_results(results_unet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcpMz0GzUJwY"
      },
      "outputs": [],
      "source": [
        "max_epochs = 80\n",
        "optimizer = torch.optim.AdamW(unet2_model.parameters(), lr=0.00100, weight_decay=0.07)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20,25,30,40,60,65,70], gamma=0.8)\n",
        "results_unet_new = train(unet2_model, optimizer, scheduler, bce_loss, score_model, max_epochs, data_tr, data_val, DEVICE)\n",
        "results_unet_new = ([t.cpu() for t in results_unet_new[0]],\n",
        "                        [t.cpu() for t in results_unet_new[1]],\n",
        "                        [t for t in results_unet_new[2]],\n",
        "                        [t for t in results_unet_new[3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJsyhecIUJwY"
      },
      "outputs": [],
      "source": [
        "score_model(unet2_model, bce_loss, data_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDvpZ7gGUJwY"
      },
      "outputs": [],
      "source": [
        "plot_results(results_unet_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZxtnTZ5f9vk"
      },
      "outputs": [],
      "source": [
        "plot_items(unet2_model, data_ts, 5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uvBUdbj1Tbro",
        "CReAQZr7TbsZ"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}